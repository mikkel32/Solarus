from __future__ import annotations

import builtins
from typing import Any, Iterable, Iterator, Mapping, Sequence, Tuple

from . import amp, cuda, nn, optim, utils


class device:
    type: str
    index: int | None

    def __init__(self, type: str, index: int | None = ...) -> None: ...


class dtype:
    ...


class Size(Sequence[int]):
    ...


class Tensor:
    def __add__(self, other: Any) -> Tensor: ...
    def __radd__(self, other: Any) -> Tensor: ...
    def __sub__(self, other: Any) -> Tensor: ...
    def __rsub__(self, other: Any) -> Tensor: ...
    def __mul__(self, other: Any) -> Tensor: ...
    def __rmul__(self, other: Any) -> Tensor: ...
    def __matmul__(self, other: Any) -> Tensor: ...
    def __rmatmul__(self, other: Any) -> Tensor: ...
    def __truediv__(self, other: Any) -> Tensor: ...
    def __rtruediv__(self, other: Any) -> Tensor: ...
    def __eq__(self, other: Any) -> Any: ...
    def __ne__(self, other: Any) -> Any: ...
    def __lt__(self, other: Any) -> Tensor: ...
    def __le__(self, other: Any) -> Tensor: ...
    def __gt__(self, other: Any) -> Tensor: ...
    def __ge__(self, other: Any) -> Tensor: ...
    def __abs__(self) -> Tensor: ...
    def __neg__(self) -> Tensor: ...
    def __iter__(self) -> Iterator[Tensor]: ...
    def __len__(self) -> int: ...
    def __getitem__(self, key: Any) -> Tensor: ...
    def __setitem__(self, key: Any, value: Any) -> None: ...
    def to(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def cpu(self) -> Tensor: ...
    def cuda(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def detach(self) -> Tensor: ...
    def clone(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def reshape(self, *shape: int) -> Tensor: ...
    def view(self, *shape: int) -> Tensor: ...
    def unsqueeze(self, dim: int) -> Tensor: ...
    def squeeze(self, dim: int | None = ...) -> Tensor: ...
    def dim(self) -> int: ...
    def numel(self) -> int: ...
    def size(self, *dims: Any) -> Any: ...
    def masked_fill(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def clamp(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def clamp_(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def clamp_min(self, min: Any) -> Tensor: ...
    def clamp_max(self, max: Any) -> Tensor: ...
    def copy_(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def is_pinned(self) -> builtins.bool: ...
    def pin_memory(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def t(self) -> Tensor: ...
    def unique(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def numpy(self) -> Any: ...
    @property
    def shape(self) -> Tuple[int, ...]: ...
    @property
    def T(self) -> Tensor: ...
    def mean(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def sum(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def max(self, *args: Any, **kwargs: Any) -> Any: ...
    def min(self, *args: Any, **kwargs: Any) -> Any: ...
    def item(self) -> float: ...
    def tolist(self) -> list[Any]: ...
    def any(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def all(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def argmax(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def softmax(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def log_softmax(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def exp(self) -> Tensor: ...
    def log(self) -> Tensor: ...
    def sqrt(self) -> Tensor: ...
    def abs(self) -> Tensor: ...
    def pow(self, exponent: Any) -> Tensor: ...
    def norm(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def mul_(self, other: Any) -> Tensor: ...
    def add_(self, other: Any) -> Tensor: ...
    def backward(self, *args: Any, **kwargs: Any) -> None: ...
    def float(self) -> Tensor: ...
    def half(self) -> Tensor: ...
    def double(self) -> Tensor: ...
    def bool(self) -> Tensor: ...
    @property
    def dtype(self) -> dtype: ...
    @property
    def device(self) -> device: ...


float32: dtype
float16: dtype
bfloat16: dtype
long: dtype
int64: dtype
bool: dtype
contiguous_format: Any


def tensor(*args: Any, **kwargs: Any) -> Tensor: ...
def zeros(*args: Any, **kwargs: Any) -> Tensor: ...
def ones(*args: Any, **kwargs: Any) -> Tensor: ...
def zeros_like(*args: Any, **kwargs: Any) -> Tensor: ...
def ones_like(*args: Any, **kwargs: Any) -> Tensor: ...
def full(*args: Any, **kwargs: Any) -> Tensor: ...
def arange(*args: Any, **kwargs: Any) -> Tensor: ...
def rand(*args: Any, **kwargs: Any) -> Tensor: ...
def randint(*args: Any, **kwargs: Any) -> Tensor: ...
def manual_seed(seed: int) -> None: ...
def set_grad_enabled(mode: builtins.bool) -> None: ...
def no_grad() -> Any: ...
def grad_enable() -> Any: ...
def from_numpy(obj: Any) -> Tensor: ...
def stack(tensors: Sequence[Tensor], *args: Any, **kwargs: Any) -> Tensor: ...
def cat(tensors: Sequence[Tensor], *args: Any, **kwargs: Any) -> Tensor: ...
def softmax(input: Tensor, dim: int, *args: Any, **kwargs: Any) -> Tensor: ...
def log_softmax(input: Tensor, dim: int, *args: Any, **kwargs: Any) -> Tensor: ...
def sigmoid(tensor: Tensor) -> Tensor: ...
def tanh(tensor: Tensor) -> Tensor: ...
def relu(tensor: Tensor) -> Tensor: ...
def expm1(tensor: Tensor) -> Tensor: ...
def log1p(tensor: Tensor) -> Tensor: ...
def clip(tensor: Tensor, *args: Any, **kwargs: Any) -> Tensor: ...
def save(obj: Any, f: Any, *args: Any, **kwargs: Any) -> None: ...
def load(f: Any, *args: Any, **kwargs: Any) -> Any: ...
def as_tensor(data: Any, *args: Any, **kwargs: Any) -> Tensor: ...
def clip_grad_norm_(
    parameters: Iterable[Tensor] | Sequence[Tensor],
    max_norm: float,
    *args: Any,
    **kwargs: Any,
) -> Tensor: ...
