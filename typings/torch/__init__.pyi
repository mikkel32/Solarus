from __future__ import annotations

from typing import Any, Iterable, Sequence, Tuple

class device:
    type: str

class dtype:
    ...

class Size(Sequence[int]):
    ...

class Tensor:
    def __add__(self, other: Any) -> Tensor: ...
    def __sub__(self, other: Any) -> Tensor: ...
    def __mul__(self, other: Any) -> Tensor: ...
    def __truediv__(self, other: Any) -> Tensor: ...
    def __iter__(self) -> Iterable[Tensor]: ...
    def __len__(self) -> int: ...
    def to(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def cpu(self) -> Tensor: ...
    def cuda(self) -> Tensor: ...
    def detach(self) -> Tensor: ...
    def clone(self) -> Tensor: ...
    def reshape(self, *shape: int) -> Tensor: ...
    def view(self, *shape: int) -> Tensor: ...
    def unsqueeze(self, dim: int) -> Tensor: ...
    def squeeze(self, dim: int | None = ...) -> Tensor: ...
    def dim(self) -> int: ...
    def numel(self) -> int: ...
    def size(self, *dims: Any) -> Any: ...
    @property
    def shape(self) -> Tuple[int, ...]: ...
    def mean(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def sum(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def max(self, *args: Any, **kwargs: Any) -> Any: ...
    def min(self, *args: Any, **kwargs: Any) -> Any: ...
    def item(self) -> float: ...
    def tolist(self) -> list[Any]: ...
    def argmax(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def softmax(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def log_softmax(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def exp(self) -> Tensor: ...
    def log(self) -> Tensor: ...
    def sqrt(self) -> Tensor: ...
    def pow(self, exponent: Any) -> Tensor: ...
    def clip_(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def norm(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def mul_(self, other: Any) -> Tensor: ...
    def add_(self, other: Any) -> Tensor: ...
    def clamp_(self, *args: Any, **kwargs: Any) -> Tensor: ...
    def clamp_min(self, min: Any) -> Tensor: ...
    def clamp_max(self, max: Any) -> Tensor: ...
    def backward(self, *args: Any, **kwargs: Any) -> None: ...
    def float(self) -> Tensor: ...
    def half(self) -> Tensor: ...
    def double(self) -> Tensor: ...
    def bool(self) -> Tensor: ...
    @property
    def dtype(self) -> dtype: ...
    @property
    def device(self) -> device: ...

float32: dtype
float16: dtype
bfloat16: dtype
long: dtype
int64: dtype
bool: dtype


def tensor(*args: Any, **kwargs: Any) -> Tensor: ...
def zeros(*args: Any, **kwargs: Any) -> Tensor: ...
def ones(*args: Any, **kwargs: Any) -> Tensor: ...
def full(*args: Any, **kwargs: Any) -> Tensor: ...
def arange(*args: Any, **kwargs: Any) -> Tensor: ...
def rand(*args: Any, **kwargs: Any) -> Tensor: ...
def randint(*args: Any, **kwargs: Any) -> Tensor: ...
def manual_seed(seed: int) -> None: ...
def set_grad_enabled(mode: bool) -> None: ...
def no_grad() -> Any: ...
def grad_enable() -> Any: ...
def cuda() -> Any: ...
def device(type: str, index: int | None = ...) -> device: ...
def is_available() -> bool: ...
def from_numpy(obj: Any) -> Tensor: ...
def stack(tensors: Sequence[Tensor], *args: Any, **kwargs: Any) -> Tensor: ...
def cat(tensors: Sequence[Tensor], *args: Any, **kwargs: Any) -> Tensor: ...
def softmax(input: Tensor, dim: int, *args: Any, **kwargs: Any) -> Tensor: ...
def log_softmax(input: Tensor, dim: int, *args: Any, **kwargs: Any) -> Tensor: ...
def sigmoid(tensor: Tensor) -> Tensor: ...
def tanh(tensor: Tensor) -> Tensor: ...
def relu(tensor: Tensor) -> Tensor: ...
def expm1(tensor: Tensor) -> Tensor: ...
def log1p(tensor: Tensor) -> Tensor: ...
def clip(tensor: Tensor, *args: Any, **kwargs: Any) -> Tensor: ...
